knitr::opts_chunk$set(echo = TRUE)
library(mlbench)
library(ggplot2)
library(beeswarm)
data(BostonHousing)
head(BostonHousing)
str(BostonHousing)
# checking for null values
colSums(is.na(BostonHousing))
summary(BostonHousing)
avg<-colMeans(BostonHousing[-4])
barplot(avg, main="Averages of Each column in Data")
boxplot(BostonHousing[-c(4,7,10,12)])
boxplot(BostonHousing[c(7,10,12)])
# choosing to plot medv vs remaining columns in dataset
price <- "medv"
# create a for loop to iterate over each column in the data frame
plots_list <- lapply(names(BostonHousing), function(var) {
if (var != price) {
ggplot(BostonHousing, aes_string(x = var, y = price)) +
geom_point() +
labs(x = var, y = price) +
theme_bw()
} else {
NULL
}
})
library(gridExtra)
grid.arrange(grobs = plots_list, ncol = 5)
ggplot(BostonHousing) +
aes(x = rm, y = medv) +
geom_point(shape = "circle", size = 1.5, colour = "#112446") +
theme_minimal()
ggplot(BostonHousing) +
aes(x = medv, y = tax) +
geom_point(shape = "circle", size = 1.5, colour = "#112446") +
labs(x = "Price", y = "Tax", title = "Price vs Tax") +
theme_minimal()
beeswarm(BostonHousing$medv~BostonHousing$chas)
library(corrplot)
# correlation plot using the corrplot function
corrplot(cor(BostonHousing[-4]), method = "color", type = "upper", order = "hclust", addCoef.col = "black")
model_bh <- lm(medv~.,data=BostonHousing)
anova(model_bh)
library(ISLR)
library(dplyr)
boston_filtered <- BostonHousing %>% select("medv","crim","zn","indus","chas","rm",
"age","dis","tax","ptratio","b",
"lstat")
set.seed(123)
library(caret)
#Partitioning Data into 80% Training and 20% Validation
Index_Train<-createDataPartition(boston_filtered$medv, p=0.8, list=FALSE)
boston_Train <-boston_filtered[Index_Train,]
boston_Validation  <-boston_filtered[-Index_Train,]
norm_model<-preProcess(boston_Train, method = c("center", "scale"))
#Applying Normalization model to all three data
boston_norm_Train <-predict(norm_model,boston_Train)
boston_norm_Validation  <-predict(norm_model,boston_Validation)
linear <- lm(medv~.,data=boston_norm_Train)
summary(linear)
library(rpart.plot)
library(rattle)
library(rpart)
DT=rpart(medv~.,data=boston_norm_Train, method='anova')
fancyRpartPlot(DT)
DT_train <- caret::train(medv~.,data=boston_norm_Train,
method = "rpart" )
DT_train
set.seed(123)
Random_forest<-train(medv~., data=boston_norm_Train,method='rf')
print(Random_forest)
set.seed(123)
svm<-train(medv~., data=boston_norm_Train,method='svmLinear')
print(svm)
library(readr)
results <- read_csv("results.csv", col_types = cols(MODEL = col_factor(levels = c("Linear Regression",  "Decision Trees", "Random Forest", "SVM"))), na = "0")
results <- na.omit(results)
ggplot(results) + aes(x = MODEL, fill = `R- SQUARED`, weight = RMSE) + geom_bar() +
scale_fill_distiller(palette = "Blues", direction = 1) + theme_minimal()
set.seed(123)
Random_forest_2<-train(medv~., data=boston_Train,method='rf',
preProcess = c("center", "scale"))
predicted <- c(predict(Random_forest_2,boston_Validation[-1]))
output<-as.data.frame(predicted)
output$actual <- boston_Validation$medv
output
knitr::opts_chunk$set(echo = TRUE)
library(mlbench)
library(ggplot2)
library(beeswarm)
data(BostonHousing)
head(BostonHousing)
str(BostonHousing)
# checking for null values
colSums(is.na(BostonHousing))
summary(BostonHousing)
avg<-colMeans(BostonHousing[-4])
barplot(avg, main="Averages of Each column in Data")
boxplot(BostonHousing[-c(4,7,10,12)])
boxplot(BostonHousing[c(7,10,12)])
# choosing to plot medv vs remaining columns in dataset
price <- "medv"
# create a for loop to iterate over each column in the data frame
plots_list <- lapply(names(BostonHousing), function(var) {
if (var != price) {
ggplot(BostonHousing, aes_string(x = var, y = price)) +
geom_point() +
labs(x = var, y = price) +
theme_bw()
} else {
NULL
}
})
library(gridExtra)
grid.arrange(grobs = plots_list, ncol = 5)
ggplot(BostonHousing) +
aes(x = rm, y = medv) +
geom_point(shape = "circle", size = 1.5, colour = "#112446") +
theme_minimal()
ggplot(BostonHousing) +
aes(x = medv, y = tax) +
geom_point(shape = "circle", size = 1.5, colour = "#112446") +
labs(x = "Price", y = "Tax", title = "Price vs Tax") +
theme_minimal()
beeswarm(BostonHousing$medv~BostonHousing$chas)
library(corrplot)
# correlation plot using the corrplot function
corrplot(cor(BostonHousing[-4]), method = "color", type = "upper", order = "hclust", addCoef.col = "black")
model_bh <- lm(medv~.,data=BostonHousing)
anova(model_bh)
library(ISLR)
library(dplyr)
boston_filtered <- BostonHousing %>% select("medv","crim","zn","indus","chas","rm",
"age","dis","tax","ptratio","b",
"lstat")
set.seed(123)
library(caret)
#Partitioning Data into 80% Training and 20% Validation
Index_Train<-createDataPartition(boston_filtered$medv, p=0.8, list=FALSE)
boston_Train <-boston_filtered[Index_Train,]
boston_Validation  <-boston_filtered[-Index_Train,]
norm_model<-preProcess(boston_Train, method = c("center", "scale"))
#Applying Normalization model to all three data
boston_norm_Train <-predict(norm_model,boston_Train)
boston_norm_Validation  <-predict(norm_model,boston_Validation)
linear <- lm(medv~.,data=boston_norm_Train)
summary(linear)
library(rpart.plot)
library(rattle)
library(rpart)
DT=rpart(medv~.,data=boston_norm_Train, method='anova')
fancyRpartPlot(DT)
DT_train <- caret::train(medv~.,data=boston_norm_Train,
method = "rpart" )
DT_train
set.seed(123)
Random_forest<-train(medv~., data=boston_norm_Train,method='rf')
print(Random_forest)
set.seed(123)
svm<-train(medv~., data=boston_norm_Train,method='svmLinear')
print(svm)
library(readr)
results <- read_csv("results.csv", col_types = cols(MODEL = col_factor(levels = c("Linear Regression",  "Decision Trees", "Random Forest", "SVM"))), na = "0")
results <- na.omit(results)
ggplot(results) + aes(x = MODEL, fill = `R- SQUARED`, weight = RMSE) + geom_bar() +
scale_fill_distiller(palette = "Blues", direction = 1) + theme_minimal()
set.seed(123)
Random_forest_2<-train(medv~., data=boston_Train,method='rf',
preProcess = c("center", "scale"))
predicted <- c(predict(Random_forest_2,boston_Validation[-1]))
output<-as.data.frame(predicted)
output$actual <- boston_Validation$medv
output
knitr::opts_chunk$set(echo = TRUE)
library(mlbench)
library(ggplot2)
library(beeswarm)
data(BostonHousing)
head(BostonHousing)
str(BostonHousing)
# checking for null values
colSums(is.na(BostonHousing))
summary(BostonHousing)
avg<-colMeans(BostonHousing[-4])
barplot(avg, main="Averages of Each column in Data")
boxplot(BostonHousing[-c(4,7,10,12)])
boxplot(BostonHousing[c(7,10,12)])
# choosing to plot medv vs remaining columns in dataset
price <- "medv"
# create a for loop to iterate over each column in the data frame
plots_list <- lapply(names(BostonHousing), function(var) {
if (var != price) {
ggplot(BostonHousing, aes_string(x = var, y = price)) +
geom_point() +
labs(x = var, y = price) +
theme_bw()
} else {
NULL
}
})
library(gridExtra)
grid.arrange(grobs = plots_list, ncol = 5)
ggplot(BostonHousing) +
aes(x = rm, y = medv) +
geom_point(shape = "circle", size = 1.5, colour = "#112446") +
theme_minimal()
ggplot(BostonHousing) +
aes(x = medv, y = tax) +
geom_point(shape = "circle", size = 1.5, colour = "#112446") +
labs(x = "Price", y = "Tax", title = "Price vs Tax") +
theme_minimal()
beeswarm(BostonHousing$medv~BostonHousing$chas)
library(corrplot)
# correlation plot using the corrplot function
corrplot(cor(BostonHousing[-4]), method = "color", type = "upper", order = "hclust", addCoef.col = "black")
model_bh <- lm(medv~.,data=BostonHousing)
anova(model_bh)
library(ISLR)
library(dplyr)
boston_filtered <- BostonHousing %>% select("medv","crim","zn","indus","chas","rm",
"age","dis","tax","ptratio","b",
"lstat")
set.seed(123)
library(caret)
#Partitioning Data into 80% Training and 20% Validation
Index_Train<-createDataPartition(boston_filtered$medv, p=0.8, list=FALSE)
boston_Train <-boston_filtered[Index_Train,]
boston_Validation  <-boston_filtered[-Index_Train,]
norm_model<-preProcess(boston_Train, method = c("center", "scale"))
#Applying Normalization model to all three data
boston_norm_Train <-predict(norm_model,boston_Train)
boston_norm_Validation  <-predict(norm_model,boston_Validation)
linear <- lm(medv~.,data=boston_norm_Train)
summary(linear)
library(rpart.plot)
library(rattle)
library(rpart)
DT=rpart(medv~.,data=boston_norm_Train, method='anova')
fancyRpartPlot(DT)
DT_train <- caret::train(medv~.,data=boston_norm_Train,
method = "rpart" )
DT_train
set.seed(123)
Random_forest<-train(medv~., data=boston_norm_Train,method='rf')
print(Random_forest)
set.seed(123)
svm<-train(medv~., data=boston_norm_Train,method='svmLinear')
print(svm)
library(readr)
results <- read_csv("results.csv", col_types = cols(MODEL = col_factor(levels = c("Linear Regression",  "Decision Trees", "Random Forest", "SVM"))), na = "0")
results <- na.omit(results)
ggplot(results) + aes(x = MODEL, fill = `R- SQUARED`, weight = RMSE) + geom_bar() +
scale_fill_distiller(palette = "Blues", direction = 1) + theme_minimal()
set.seed(123)
Random_forest_2<-train(medv~., data=boston_Train,method='rf',
preProcess = c("center", "scale"))
predicted <- c(predict(Random_forest_2,boston_Validation[-1]))
output<-as.data.frame(predicted)
output$actual <- boston_Validation$medv
output
